{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "9145f995-badc-459a-a8af-e793ab2dcbcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Classification Report NB:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "     business       0.89      0.89      0.89        84\n",
      "entertainment       0.85      0.88      0.87        60\n",
      "     politics       0.89      0.89      0.89        65\n",
      "        sport       1.00      0.94      0.97        69\n",
      "         tech       0.89      0.91      0.90        68\n",
      "\n",
      "     accuracy                           0.90       346\n",
      "    macro avg       0.91      0.90      0.90       346\n",
      " weighted avg       0.91      0.90      0.91       346\n",
      "\n",
      "\n",
      "Classification Report RF:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "     business       0.86      0.99      0.92        84\n",
      "entertainment       1.00      0.85      0.92        60\n",
      "     politics       0.97      0.98      0.98        65\n",
      "        sport       0.96      0.99      0.97        69\n",
      "         tech       0.97      0.88      0.92        68\n",
      "\n",
      "     accuracy                           0.94       346\n",
      "    macro avg       0.95      0.94      0.94       346\n",
      " weighted avg       0.95      0.94      0.94       346\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n#Creating a per-category accuracy report\\ndef per_category_accuracy(y_true, y_pred):\\n    #Extract unique categories form the true labels and sort them\\n    categories = sorted(set(y_true))\\n    #Initialize a dictionary to hold accuracy metrics for each category\\n    accuracy_per_category = {}\\n    \\n    #Loop over each unique category to evaluate its perormance\\n    for category in categories:\\n        #Count the total number of actual instances of this category in the true labels\\n        actual_count = (y_true == category).sum() \\n        #Count the number of instances correctly predicted as this category\\n        correct_count = ((y_true == category) & (y_pred == category)).sum()\\n        \\n        #Calculate the accuracy for the current category\\n        #Avoid division by zero by checking if actual_count os greater than zero\\n        accuracy = correct_count / actual_count if actual_count > 0 else 0\\n       #Store the results in the dictionary with detailed metrics\\n        accuracy_per_category[category] = {\\n            \\'Total\\': actual_count,\\n            \\'Correct\\': correct_count,\\n            \\'Accuracy\\': accuracy\\n        }\\n    #Convert the accuracy dictionary into a Pandas DataFrame for better display\\n    return pd.DataFrame(accuracy_per_category).T\\n\\n#Call the function to generate the per-category accuracy report using the true and predicted labels\\ncategory_accuracy_report = per_category_accuracy(y_test, bot_predictions)\\n\\n#Print the per-category accuracy report to analyze the model performance on each class\\nprint(\"\\nPer-Category Accuracy Report:\")\\nprint(category_accuracy_report)\\n\\n#Visualization: Create a confusion Matrix to viaualize the performance of theclassification model\\n#The confusion matrix summarizes the count of correct and incorrect predictions, which helps identify areas of improvement\\nconfusion_mat = confusion_matrix(y_test, bot_predictions)\\n\\n#Set up the visual parameters for the confusion matrix plot\\nplt.figure(figsize=(10, 7))\\n\\n#Get and sort unique categories for label display in the confusion matrix\\nunique_categories = sorted(set(y))\\n\\n#Create a heatmap to represent the confusion matrix visually\\n#Use annotations to display the counts in each cell with a blue color palette\\nsns.heatmap(confusion_mat, annot=True, fmt=\\'d\\', cmap=\\'Blues\\', xticklabels=sorted(set(y)), yticklabels=sorted(set(y)))\\n\\n#Label the axes to clarify what is being represented in the confusion matrix\\nplt.ylabel(\\'Actual\\')\\nplt.xlabel(\\'Predicted\\')\\nplt.title(\\'Confusion Matrix\\')\\nplt.show()\\n'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import joblib\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "#Convert the CSV files to dataframes. \n",
    "data= pd.read_csv('lemmatized_articles.csv')\n",
    "data2= pd.read_csv('BBC_train_full.csv')\n",
    "\n",
    "#Gets the tokenized data\n",
    "x = data['tokens']\n",
    "#Gets the labels\n",
    "y = data2['category'].values\n",
    "\n",
    "#This converts the tokens to a vector so it can be converted to an array later to be read by the bot\n",
    "vectorizer = CountVectorizer()\n",
    "x_transformed = vectorizer.fit_transform(x)\n",
    "joblib.dump(vectorizer, 'vectorizer.joblib')\n",
    "\n",
    "#Sets the random seed. Some potential numbers to test: 40, 52, 53, 605, 6010, \n",
    "seed = 41\n",
    "\n",
    "#Splits the data into 20% test and 80% train. Random state is the seed used for the randomization.\n",
    "x_train, x_test, y_train, y_test = train_test_split(x_transformed, y, test_size=0.2, random_state=seed)\n",
    "\n",
    "#Further splits the 80% data into two sets of 40%. This is so we can train the two bots with the same data set.\n",
    "x_train_bot1, x_train_bot2, y_train_bot1, y_train_bot2 = train_test_split(x_train, y_train, test_size=0.5, random_state=seed)\n",
    "\n",
    "#Creates the bots\n",
    "NBBot = GaussianNB()\n",
    "RFBot = RandomForestClassifier(random_state=seed)\n",
    "\n",
    "\n",
    "#This is where we 'train' the bots, by giving them things to reference\n",
    "NBBot.fit(x_train_bot1.toarray(), y_train_bot1)\n",
    "RFBot.fit(x_train_bot2, y_train_bot2)\n",
    "\n",
    "#This stores the bots predictions. We also convert x_test to an array so it can be read correctly\n",
    "NB_predictions = NBBot.predict(x_test.toarray())\n",
    "RF_predictions= RFBot.predict(x_test)\n",
    "\n",
    "#We get the accuray score by compparing the predictions to the labels\n",
    "accuracyNB = accuracy_score(y_test, NB_predictions)\n",
    "accuracyRF = accuracy_score(y_test, RF_predictions)\n",
    "\n",
    "#Saves the bots for reuse later\n",
    "joblib.dump(NBBot, 'NBBotML.joblib')\n",
    "joblib.dump(RFBot, 'RFBotML.joblib')\n",
    "\n",
    "\n",
    "# Load the model\n",
    "#loaded_bot = joblib.load('NBBot.joblib')\n",
    "\n",
    "# Use the loaded model for predictions\n",
    "#loaded_predictions = loaded_bot.predict(x_test.toarray())\n",
    "\n",
    "\n",
    "#Classification report\n",
    "#This report provides detailed metrics about the model's performance\n",
    "#It includes precision, recall, F1-score, and support for each class\n",
    "print(\"\\nClassification Report NB:\")\n",
    "#Generate the report by comparing true labels ('y_test') with predicted labels ('bot_predictions')\n",
    "print(classification_report(y_test, NB_predictions))\n",
    "print(\"\\nClassification Report RF:\")\n",
    "print(classification_report(y_test, RF_predictions))\n",
    "\n",
    "\n",
    "'''\n",
    "#Creating a per-category accuracy report\n",
    "def per_category_accuracy(y_true, y_pred):\n",
    "    #Extract unique categories form the true labels and sort them\n",
    "    categories = sorted(set(y_true))\n",
    "    #Initialize a dictionary to hold accuracy metrics for each category\n",
    "    accuracy_per_category = {}\n",
    "    \n",
    "    #Loop over each unique category to evaluate its perormance\n",
    "    for category in categories:\n",
    "        #Count the total number of actual instances of this category in the true labels\n",
    "        actual_count = (y_true == category).sum() \n",
    "        #Count the number of instances correctly predicted as this category\n",
    "        correct_count = ((y_true == category) & (y_pred == category)).sum()\n",
    "        \n",
    "        #Calculate the accuracy for the current category\n",
    "        #Avoid division by zero by checking if actual_count os greater than zero\n",
    "        accuracy = correct_count / actual_count if actual_count > 0 else 0\n",
    "       #Store the results in the dictionary with detailed metrics\n",
    "        accuracy_per_category[category] = {\n",
    "            'Total': actual_count,\n",
    "            'Correct': correct_count,\n",
    "            'Accuracy': accuracy\n",
    "        }\n",
    "    #Convert the accuracy dictionary into a Pandas DataFrame for better display\n",
    "    return pd.DataFrame(accuracy_per_category).T\n",
    "\n",
    "#Call the function to generate the per-category accuracy report using the true and predicted labels\n",
    "category_accuracy_report = per_category_accuracy(y_test, bot_predictions)\n",
    "\n",
    "#Print the per-category accuracy report to analyze the model performance on each class\n",
    "print(\"\\nPer-Category Accuracy Report:\")\n",
    "print(category_accuracy_report)\n",
    "\n",
    "#Visualization: Create a confusion Matrix to viaualize the performance of theclassification model\n",
    "#The confusion matrix summarizes the count of correct and incorrect predictions, which helps identify areas of improvement\n",
    "confusion_mat = confusion_matrix(y_test, bot_predictions)\n",
    "\n",
    "#Set up the visual parameters for the confusion matrix plot\n",
    "plt.figure(figsize=(10, 7))\n",
    "\n",
    "#Get and sort unique categories for label display in the confusion matrix\n",
    "unique_categories = sorted(set(y))\n",
    "\n",
    "#Create a heatmap to represent the confusion matrix visually\n",
    "#Use annotations to display the counts in each cell with a blue color palette\n",
    "sns.heatmap(confusion_mat, annot=True, fmt='d', cmap='Blues', xticklabels=sorted(set(y)), yticklabels=sorted(set(y)))\n",
    "\n",
    "#Label the axes to clarify what is being represented in the confusion matrix\n",
    "plt.ylabel('Actual')\n",
    "plt.xlabel('Predicted')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()\n",
    "'''\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09fee611-b786-4293-8fb9-dc154d31076c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
